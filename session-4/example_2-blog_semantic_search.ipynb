{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44353ae1-41a5-4e8e-bdfb-fec1f445f239",
   "metadata": {},
   "source": [
    "# Semantic Search with Text Embeddings\n",
    "## ABB #2 - Session 4\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22ee10-b6c8-4e59-babd-366b41f4a357",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c3df73-038b-44b9-9bf2-526dc485c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667ac64-c7f7-4a29-9bf4-9eda692145c8",
   "metadata": {},
   "source": [
    "### 1) chunk articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53d05bc-7ace-455a-a6bd-bdf45116d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all HTML files from raw directory\n",
    "filename_list = [\"articles/\"+f for f in os.listdir('articles')]\n",
    "\n",
    "chunk_list = []\n",
    "for filename in filename_list:\n",
    "    # only process .html files\n",
    "    if filename.lower().endswith(('.html')):\n",
    "        # read html file\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "    \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Get article title\n",
    "        article_title = soup.find('title').get_text().strip() if soup.find('title') else \"Untitled\"\n",
    "        \n",
    "        # Initialize variables\n",
    "        article_content = []\n",
    "        current_section = \"Main\"  # Default section if no headers found\n",
    "        \n",
    "        # Find all headers and text content\n",
    "        content_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'ul', 'ol'])\n",
    "    \n",
    "        # iterate through elements and extract text with metadata\n",
    "        for element in content_elements:\n",
    "            if element.name in ['h1', 'h2', 'h3']:\n",
    "                current_section = element.get_text().strip()\n",
    "            elif element.name in ['p', 'ul', 'ol']:\n",
    "                text = element.get_text().strip()\n",
    "                # Only add non-empty content that's at least 30 characters long\n",
    "                if text and len(text) >= 30:\n",
    "                    article_content.append({\n",
    "                        'article_title': article_title,\n",
    "                        'section': current_section,\n",
    "                        'text': text\n",
    "                    })\n",
    "    \n",
    "        # add article content to list\n",
    "        chunk_list.extend(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70b7fbf-c363-4a5a-84b8-be342a450dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chunk list to file\n",
    "filename='data/chunk_list.json'\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50b033-8519-4b93-b8ac-b05eed841604",
   "metadata": {},
   "source": [
    "### 2) embed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e1ddfa-8691-4cb0-8eef-74d7dc7db933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num chunks: 778\n"
     ]
    }
   ],
   "source": [
    "# define text to embed\n",
    "text_list = []\n",
    "for content in chunk_list:\n",
    "    # concatenate title and section header\n",
    "    context = content['article_title'] + \" - \" + content['section'] + \": \"\n",
    "    # append text from paragraph to fill CLIP's 256 sequence limit\n",
    "    text = context + content['text'][:512-len(context)]\n",
    "    \n",
    "    text_list.append(text)\n",
    "print(\"Num chunks:\",len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd448c0-f1f2-476d-b6cf-415b19cf399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': 'Fine-Tuning BERT for Text Classification',\n",
       " 'section': 'Fine-Tuning BERT for Text Classification',\n",
       " 'text': 'Although today’s 100B+ parameter transformer models are state-of-the-art in AI, there’s still much we can accomplish with smaller (< 1B parameter) models. In this article, I will walk through one such example, fine-tuning BERT (110M parameters) to classify phishing URLs. I’ll start by covering key concepts and then share example Python code.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450f026c-153d-4894-a03e-9eda2ee32455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fine-Tuning BERT for Text Classification - Fine-Tuning BERT for Text Classification: Although today’s 100B+ parameter transformer models are state-of-the-art in AI, there’s still much we can accomplish with smaller (< 1B parameter) models. In this article, I will walk through one such example, fine-tuning BERT (110M parameters) to classify phishing URLs. I’ll start by covering key concepts and then share example Python code.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03fe32a-230a-416a-ac7b-a25d85a41b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778, 384)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = SentenceTransformer(\"multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "# compute embeddings\n",
    "chunk_embeddings = model.encode(text_list)\n",
    "print(chunk_embeddings.shape)\n",
    "\n",
    "# save chunk embeddings to file\n",
    "torch.save(chunk_embeddings, 'data/chunk_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9ac6-134e-421c-904e-ec6bf81834db",
   "metadata": {},
   "source": [
    "### 3) semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43e73516-fdfd-4963-ad19-2c3e413ab5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n",
      "torch.Size([1, 778])\n"
     ]
    }
   ],
   "source": [
    "# define query\n",
    "query = \"What is a token?\"\n",
    "query_embedding = model.encode(query)\n",
    "print(query_embedding.shape)\n",
    "\n",
    "# compute similarity between query and all chunks\n",
    "similarities = model.similarity(query_embedding, chunk_embeddings)\n",
    "print(similarities.shape)\n",
    "# print(similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3aaf616-c522-45c1-a89b-3af6ae4166bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search parameters\n",
    "temp = 0.1\n",
    "k=3\n",
    "threshold = 0.05\n",
    "\n",
    "# Rescale similarities via softmax\n",
    "scores = torch.nn.functional.softmax(similarities/temp, dim=1)\n",
    "\n",
    "# Get sorted indices and scores\n",
    "sorted_indices = scores.argsort(descending=True)[0]\n",
    "sorted_scores = scores[0][sorted_indices]\n",
    "\n",
    "# Filter by threshold and get top k\n",
    "filtered_indices = [\n",
    "    idx.item() for idx, score in zip(sorted_indices, sorted_scores) \n",
    "    if score.item() >= threshold\n",
    "][:k]\n",
    "\n",
    "# Get corresponding content items and scores\n",
    "top_results = [chunk_list[i] for i in filtered_indices]\n",
    "result_scores = [scores[0][i].item() for i in filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033e5528-44a0-4e7c-8cec-9eb4baeb77b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article_title': 'Cracking Open the OpenAI (Python) API',\n",
       "  'section': '2) OpenAI’s (Python)\\xa0API',\n",
       "  'text': 'Tokens, in the context of LLMs, are essentially a set of numbers representing a set of words and characters. For example, “The” could be a token, “ end” (with the space) could be another, and “.” another.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d785f-a1df-40aa-b762-2b79f3748df5",
   "metadata": {},
   "source": [
    "### 4) display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246b8d0f-beea-406b-81b0-484171fb6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_markdown = \"\"\n",
    "for i, result in enumerate(top_results, start=1):\n",
    "    results_markdown += f\"{i}. **Article title:** {result['article_title']}  \\n\"\n",
    "    results_markdown += f\"   **Section:** {result['section']}  \\n\"\n",
    "    results_markdown += f\"   **Snippet:** {result['text']}  \\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97df4231-af86-4dd3-b806-cab95bb4b5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** Cracking Open the OpenAI (Python) API  \n",
       "   **Section:** 2) OpenAI’s (Python) API  \n",
       "   **Snippet:** Tokens, in the context of LLMs, are essentially a set of numbers representing a set of words and characters. For example, “The” could be a token, “ end” (with the space) could be another, and “.” another.  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b101df6f-93a0-4a32-a362-e067397a9140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** We’ve already mentioned situations where RAG and fine-tuning perform well. However, since this is such a common question, it’s worth reemphasizing when each approach works best.  \n",
       "\n",
       "2. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** RAG is when we inject relevant context into an LLM’s input prompt so that it can generate more helpful responses. For example, if we have a domain-specific knowledge base (e.g., internal company documents and emails), we might identify the items most relevant to the user’s query so that an LLM can synthesize information in an accurate and digestible way.  \n",
       "\n",
       "3. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Here’s high-level guidance on when to use each.  \n",
       "\n",
       "4. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** Notice that these approaches are not mutually exclusive. In fact, the original RAG system proposed by Facebook researchers used fine-tuning to better use retrieved information for generating responses [4].  \n",
       "\n",
       "5. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Why we care  \n",
       "   **Snippet:** Previous articles in this series discussed fine-tuning, which adapts an existing model for a particular use case. While this is an alternative way to endow an LLM with specialized knowledge, empirically, fine-tuning seems to be less effective than RAG at doing this [1].  \n",
       "\n",
       "6. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Some Nuances  \n",
       "   **Snippet:** Document preparation—The quality of a RAG system is driven by how well useful information can be extracted from source documents. For example, if a document is unformatted and full of images and tables, it will be more difficult to parse than a well-formatted text file.  \n",
       "\n",
       "7. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** Some Nuances  \n",
       "   **Snippet:** While the steps for building a RAG system are conceptually simple, several nuances can make building one (in the real world) more complicated.  \n",
       "\n",
       "8. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** When NOT to Fine-tune  \n",
       "   **Snippet:** The effectiveness of any approach will depend on the details of the use case. For example, fine-tuning is less effective than retrieval augmented generation (RAG) to provide LLMs with specialized knowledge [1].  \n",
       "\n",
       "9. **Article title:** LLM Fine-tuning — FAQs  \n",
       "   **Section:** RAG vs Fine-tuning?  \n",
       "   **Snippet:** RAG: necessary knowledge for the task is not commonly known or available on the web but can be stored in a databaseFine-tuning: necessary knowledge for the task is already baked into the model, but you want to reduce the prompt size or refine response qualityRAG + Fine-tuning: the task requires specialized knowledge, and we would like to reduce the prompt size or refine the response quality  \n",
       "\n",
       "10. **Article title:** How to Improve LLMs with RAG  \n",
       "   **Section:** What is RAG?  \n",
       "   **Snippet:** RAG works by adding a step to this basic process. Namely, a retrieval step is performed where, based on the user’s prompt, the relevant information is extracted from an external knowledge base and injected into the prompt before being passed to the LLM.  \n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bringing it all together\n",
    "query = \"What's the difference between RAG and Fine-tuning?\"\n",
    "results_markdown = semantic_search(query, model, chunk_embeddings, chunk_list, temp=0.1, k=10, threshold=0)\n",
    "display(Markdown(results_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390287f1-7ea2-4627-a646-e2ff9101b1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
